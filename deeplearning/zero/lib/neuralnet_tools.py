#!/usr/bin/env python3

import numpy as np

def sigmoid(x):
    """
    # シグモイド関数

    活性化関数として使う (別の選択肢として ステップ関数 `λx -> 1 if x > 0 else 0` があり)

    ※ステップ関数もシグモイド関数も非線形関数。
      ちなみに、  ニューラルネットワークでは活性化関数に非線型関数を使う必要がある
      なぜならば、線形関数の場合はどんなに層を厚くしても、それとおなじことを行う「隠れ層のないネットワーク」が存在する為、多層にする意味がなくなる

    ## 定義

    h(x) =     1
          -------------
          1 + exp( -x )

    """
    return 1 / (1 + np.exp(-x))

def softmax(a):
    """
    # ソフトマックス関数

    出力層の活性化関数。
    一般的に回帰問題ではid関数を、分類問題ではソフトマックス関数を使う。

    ## 定義

    let 
      a : Array of Real
      n = len(a)
    in 
        k ∈ [1 .. n] in

           y[k]  =                exp( a[k] ) 
                   ----------------------------------------
                    sum  <| map λi -> exp(a[i]) <| [1 .. n]


    ただし、指数関数 exp(x) によるオーバーフローを防ぐため
    定数項 C = max(a) を a[k] より引く

    式変形は以下

        y[k] =  exp(a[k])          / ( sum <| map λi -> exp(a[i])         <| [1..n] )
             = (exp(a[k]) * C)     / ( sum <| map λi -> exp(a[i]) * C     <| [1..n] )
             =  exp(a[k] + log(C)) / ( sum <| map λi -> exp(a[i] + log(C) <| [1..n] )
             =  exp(a[k] + C' )    / ( sum <| map λi -> exp(a[i] + C'     <| [1..n] )

    ## 特徴

    ソフトマックス関数は必ず、i ∈ 実数 ; (0 <= i <= 1)  と成る。
    また、その総和は1に成る

    そのため、ソフトマックス関数の出力を「確率」として扱うことが出来る。

    ソフトマックスでは各要素の大小関係は変わらないため、分類を行うときに省略することがある。
    （一番出力の大きなニューロンをクラス分類に使うため）
    """

    c = np.max(a)
    exp_a = np.exp(a -c)
    sum_exp_a = np.sum(exp_a)
    y = exp_a / sum_exp_a
    return y



